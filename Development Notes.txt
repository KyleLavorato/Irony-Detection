Pretraining:

Supervised learning with scraped twitter data
- Preprocess this so the model can get used to the normalized input
- Two pretraining datasets
- Random tweets with the word "a" or "the" | Assumed Non-Ironic for pretraining
- Random tweets with the phrase "#irony" | Assumed Ironic for pretraining
- No pretraining for irony classification, just ironic or not

Preprocessing
- Normalize data, blanking stuff like URL, names, segment hashtags, etc
- First slang dictionary pass
- First swear normalize pass
- Translate and normalize emojis to custom true meaning, not dictionary meaning
- First tokenize pass + second slang pass
- Second swear pass to get new words added from last slang pass
- Replace garbage characters '|'
- Second tokenize pass to fix spacing from removed characters

Model
- Random weights initialized for pretraining
- Pretrain weights with the extra dataset
- Refresh learning rate to train the weights for the classification
- Impose a higher penalty on the classifications with lower representation during backpropagation
